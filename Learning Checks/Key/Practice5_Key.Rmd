---
title: "Learning Check 5"
author: "Enter Name Here"
date: "`r format(Sys.time(), '%a/%d/%b')`"
output: 
  html_document:
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
p_load("utils", "lme4", "lmerTest", "parameters", "ggbeeswarm", "viridis", "performance", "readxl", install = T)
# Load additional packages here

Phlebitis_Data <- read.csv("/Users/jayjeffries/Desktop/MLM/Practice/MLMBootcamp/Practice Data/phlebitis.csv")
```

Good repeated measures data is surprisingly hard to find! This data derives from a [Penn State statistics course](https://online.stat.psu.edu/stat510/lesson/10/10.1). We will be using the fictitious data named "phlebitis.csv" to create multilevel models with repeated measures that would typically be *far* too underpowered to actually report. But this is practice!

An experiment was done on canines to examine ways to detect phlebitis during the intravenous (IV) administration of a drug. Phlebitis is an inflammation of a blood vein. Three IV treatments were administered. 15 dogs were randomly divided into one of three groups of n = 5. Each group is given a different treatment. The treatments are:

1. The drug in a solution designed to carry the drug
2. The carrier solution only (no drug)
3. Saline solution

Treatments were administered to one ear of the dog. The Y variable represents the difference in $^{\circ}F$ temperature between the treated and untreated ear. It is believed that increased temperature in the treated ear may be an early sign of phlebitis. This experiment is measured at times 0 minutes (baseline), 30 minutes, 60 minutes, and 90 minutes after the treatment administration. 

Dog age ("Age") is measured by year while dog size ("Size") is binarized as being $\leq 50$lbs = 0 and $> 50$lbs = 1. While size is categorized in this 0/1 fashion for simplicity's sake, I would recommend using the dog's actual weight or a more comprehensive scale.

Before moving into the Practice Problems, ensure that the treatment variable is a factor in your data by transforming the column Treatment and Size via code similar to `data$Treatment <- factor(data$Treatment)`, otherwise the model will treat treatments 1 through 3 as integer values.

```{r}
# Transform data here
Phlebitis_Data$Treatment <- factor(Phlebitis_Data$Treatment)
Phlebitis_Data$Time <- factor(Phlebitis_Data$Time)
```

## Practice Problem 1

Create an unconditional (null) model with difference in ear temperatures ("Y") as the outcome variable that uses "Animal" as the clustering variable. Use FIML as the estimator by inputting `REML = F` in the `lmer()` function. Then, compute the empty model's intraclass correlation with any method that we have used (`{performance}`, `VarCorr()`, or by hand). 

```{r}
# Create empty model and compute ICC
null_mod <- lmer(Y ~ 1 + (1 | Animal), data = Phlebitis_Data, REML = F)
summary(null_mod)
```

Considering the ICC, should we account for nested nature of the data? What proportion (or percent) of variance in the outcome is attributed to clustering?

```{r}
icc(null_mod)
```

|        Yes, as 32.9% of the variability in the outcome is due to clustering (ICC = .329), i.e., the dog.

Enter level 1 fixed effects of Time. Insert level 2 fixed effect of Treatment.

```{r}
# Create conditional model with new fixed effects
cond_mod <- lmer(Y ~ factor(Treatment) + Time + 1 + (1 | Animal), data = Phlebitis_Data, REML = F)
summary(cond_mod)
```

Why would Time be considered level 1 effects? What is "level 1" in this scenario?

Does the effect of treatment type on phlebitis (i.e., difference in ear temperatures) change based on time of measure? Insert a cross-level fixed effect interaction using either "*" or ":" syntax method from Practice4 to test this.

Plot the interaction via `{ggplot2}`. 

```{r, message = F}
# Interaction plot
ggplot(Phlebitis_Data, aes(x = Time, y = Y, group = Treatment, color = Treatment)) +
  geom_quasirandom(varwidth = T, color = "gray", shape = 4) +
  geom_smooth(method = "lm", se = F, aes(color = Treatment)) + 
  labs(x = "Time", y = "Difference in Ear Temperatures") + 
  ggtitle("Spaghetti Plot of Difference in Ear Temperatures Across Time By Treatment") +
  theme_minimal() + 
  theme(text = element_text(family = "Times New Roman", face = "bold", size = 12)) + 
  scale_color_viridis(discrete = T)

cond_mod2 <- lmer(Y ~ Time*factor(Treatment) + 1 + (1 | Animal), data = Phlebitis_Data, REML = F)
summary(cond_mod2)

model_parameters(cond_mod2)
standardize_parameters(cond_mod2)
```

Does effect of Treatment on Y depend on Time? Or, reversely, does effect of Time on Y depend on Treatment? 

|       The effect of Treatment depends on Time (or vice-versa). Specifically, at all 30, 60, and 90 Time points of data collection, there is a significant decrease in the effect of Treatment 2 on difference in temperature of ears, where, respectively, the slope decreases at a rate of 1.56, 1.6, and 2.24 as Time increases ($\beta_{Time30 \times Treatment2}=-1.56, SE = .608, t(45)=-2.564, p = .014; \beta_{Time60 \times Treatment2}=-1.6, SE = .608, t(45)=-2.630, p = .012; \beta_{Time90 \times Treatment2}=-2.24, SE = .608, t(45)=-3.682, p < .001$). Similarly, at 60 and 90 Time points of data collection, there is a significant decrease in the effect of Treatment 3 on difference in temperature of ears, where, respectively, the slope decreases at a rate of 1.86 and 1.92 as Time increases ($\beta_{Time60 \times Treatment3}=-1.86, SE = .608, t(45)=-3.057, p = .004 ; \beta_{Time90 \times Treatment3}=-1.92, SE = .608, t(45)=-3.156, p = .003$). For Treatment 1, as Time increased, the difference in temperature of ears also significantly increased ($\beta_{Time30}=1.62, SE = .430, t(45)=3.766, p < .001; \beta_{Time60}=2.12, SE = .430, t(45)=4.928, p = < .001; \beta_{Time90}=2.820, SE = .430, t(45)=4.928, p < .001$). This shows that Treatment 1 increases the difference in temperature of dog's ears as Time elapses, which can indicate the occurrence of phlebitis. Practictioners should avoid the use of Treatment 1.

Test whether model fit improved between the three models (unconditional, conditional with fixed effects, conditional with fixed effects and interaction) via the `anova()` function.

```{r}
anova(null_mod, cond_mod, cond_mod2, refit = F)
```

In another model, insert level 2 fixed effects "Age" and "Size". 

```{r}
# Model with Age and Size
cond_mod3 <- lmer(Y ~ Age + Size + 1 + (1 | Animal), data = Phlebitis_Data, REML = F)
summary(cond_mod3)
```

It was also hypothesized that the effect of time on differences in ear temperature depends on a dog's size. Insert a cross-level interaction to test this.

```{r}
cond_mod4 <- lmer(Y ~ Age*Size + 1 + (1 | Animal), data = Phlebitis_Data, REML = F)
summary(cond_mod4)
```

What did the level 2 fixed effects tell us about a dog's age and size? 

Test whether model fit improved between the three models (unconditional, conditional with fixed effects, conditional with fixed effects and interaction) via the `anova()` function.

```{r}
anova(null_mod, cond_mod3, cond_mod4, refit = F)
```

---

## Practice Problem 2

```{r}
# Read in the reaction experiment data reactions.xlsx file with {readxl} package
Reaction_Data <- read_xlsx("/Users/jayjeffries/Desktop/MLM/Practice/MLMBootcamp/Practice Data/reaction.xlsx")
```

The next faux dataset that we will use, reaction.xlsx, involves the use of a reaction time experiment conducted on 24 participants ("Subject"). 

Three pairs of the same images ("Cues") are randomized to appear on the screen that features a threat on either the right half of the computer screen or the left half of the copmuter screen. On the other half of the screen is a non-threatening image. Threatening images involved graphics of a masked robber pointing a gun at the participant, an attacking mountain lion, or a snake lunging toward the participant. Participants were given a three-second countdown in between each cue.

The same pairs of cues were randomly used twice -- once in color and once in black and white. Cue images that are in black and white were coded as Color = 0, whereas all images in full color were coded as Color = 1. 

Reaction time, in milliseconds, is measured by the time it takes for the participant to press down the "Q" (left-side) or "P" (right-side) keys to indicate which side of the screen the threat exists. 

For half of the sample, a distraction ("Disturb") occurred during the experiment that involved a randomly selected emoji that appears in the center of the screen when the images are revealed. 

Participants either correctly selected the threat (Correct = 1) or incorrectly assessed the threat (Correct = 0). 

Because this dataset involves a binary outcome (correct/incorrect), we must move into the **g**eneralized **l**inear **m**ixed **e**ffect **r**egression function, `glmer()`. This function comes from the same `{lme4}` package we have been using and also uses the exact same inputs as `lmer()` but allows for new families of distributions. For our case, the binomial distribution. See the code snippet below to see how new families of distributions are utilized:

```{r, eval = F}
glmer("typical lmer() formula", family = binomial, data = "data of interest")
```

Before building the generalized linear mixed effect model, transform the "Correct", "Disturb", "Cue", and "Color" variables from this dataset into factors instead of integer/numeric class variables. As done above, use syntax similar to `data$Correct <- factor(data$Correct)` to complete this.

```{r}
# Small variable transformation here
Reaction_Data$Correct <- factor(Reaction_Data$Correct)
Reaction_Data$Disturb <- factor(Reaction_Data$Disturb)
Reaction_Data$Cue <- factor(Reaction_Data$Cue)
Reaction_Data$Color <- factor(Reaction_Data$Color)
```

The research team wants to know if the ability to correctly identify the threatening image increases as number of images (Cues) are revealed. This is analagous to testing learning effects.

In addition, they would like to see if participants are better able to distinguish a threat when viewing colored images as opposed to black and white images. 

The last objective for the team is to investigate whether on-screen disturbances change the likelihood of selecting the threatening image. 

Using `glmer()`, create a mixed effect model, with grouping variable "Subject", that predicts whether or not the participant correctly identified the threatening image based on:

* If the image was in color
* Whether or not a disturbance occurred
* Whether the participant was better at identifying a threat after seeing the first first or second image (i.e., Cue)

```{r}
# First glmer() model for reactions data
glm_mod <- glmer(Correct ~ Color + Disturb + Cue + 1 + (1 | Subject), family = binomial, data = Reaction_Data)
summary(glm_mod)
```

To refer to effects in odds ratios (in place of log-odds), I would recommend using the `model_parameters()` function from the `parameters` package (which derives from the [easystats](https://github.com/easystats/easystats) collection) in place of exponentiating by hand or single function (e.g., exp(coefficient 1), exp(coefficient 2), etc.).  Use the below code to download this and run the proper function:

```{r}
if(!require(parameters)){
    install.packages("parameters")
    library(parameters)
}

model_parameters(glm_mod, exponentiate = T)
```

On average, do the odds of a participant selecting the threat significantly differ depending on whether the image is in color? If so, are participants more or less likely to detect a threat when the image is in color?

|        No, there is no significant difference between black & white or color images in terms of correctly detecting a threat.

On average, do the odds of a participant selecting the threat significantly differ based on if there are on-screen disturbances? If so, are participants more or less likely to detect a threat when there are on-screen disturbances?

|        There were significant differences in the likelihood that participants correctly detected between instances of on-screen disturbances and non-disturbances ($\beta_{Disturb}=-.244, SE = .121, OR = .78, z = -2.01, p = .044$). When on-screen disturbances were present, participants were .78 times as likely to correctly detect a threat.

On average, do the odds of a participant selecting the threat significantly differ based on cue (number of images shown)? If so, are participants more or less likely to detect a threat during the second or third image?

|        No, there is no significant difference between ordering of images (i.e., cue) in terms of correctly detecting a threat.

Test whether the effect of an on-screen disturbance *moderates* the effect of colored images on correctly identifying the threatening image in a new `glmer()` object.

```{r}
# New glmer() model
glm_mod2 <- glmer(Correct ~ Color*Disturb + Cue + 1 + (1 | Subject), family = binomial, data = Reaction_Data)
summary(glm_mod2)
```

Using the `lmer()` model, use the same set of explanatory variables used in the above model (including the interaction) to predict the amount of time it takes for the participant to press down a key (outcome variable = "Time"). 

```{r}
# New lmer() model
cond_mod5 <- lmer(Time ~ Color*Disturb + Cue + 1 + (1 | Subject), data = Reaction_Data)
summary(cond_mod5)
```

How do these explanatory variables explain or predict the length of time it takes for the participant to select the image on the screen? Were there differences in speed based on color of image, existing distractions, or number of cue?

|        This set of predictors did not significantly predict or explain the time it took for one to select an image on screen (regardless of correct or incorrect). 