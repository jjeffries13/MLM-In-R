---
title: "Learning Check 5"
author: "Enter Name Here"
date: "`r format(Sys.time(), '%a/%d/%b')`"
output: 
  html_document:
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
p_load("utils", "lme4", "sjPlot", install = T)
# Load additional packages here
```

Good repeated measures data is surprisingly hard to find! This data derives from a [Penn State statistics course](https://online.stat.psu.edu/stat510/lesson/10/10.1). We will be using the fictitious data named "phlebitis.csv" to create multilevel models with repeated measures that would typically be *far* too underpowered to actually report. But this is practice!

An experiment was done on canines to examine ways to detect phlebitis during the intravenous (IV) administration of a drug. Phlebitis is an inflammation of a blood vein. Three IV treatments were administered. 15 dogs were randomly divided into one of three groups of n = 5. Each group is given a different treatment. The treatments are:

1. The drug in a solution designed to carry the drug
2. The carrier solution only (no drug)
3. Saline solution

Treatments were administered to one ear of the dog. The Y variable represents the difference in $^{\circ}F$ temperature between the treated and untreated ear. It is believed that increased temperature in the treated ear may be an early sign of phlebitis. This experiment is measured at times 0 minutes (baseline), 30 minutes, 60 minutes, and 90 minutes after the treatment administration. 

Dog age ("Age") is measured by year while dog size ("Size") is binarized as being $\leq 50$lbs = 0 and $> 50$lbs = 1. While size is categorized in this 0/1 fashion for simplicity's sake, I would recommend using the dog's actual weight or a more comprehensive scale.

Before moving into the Practice Problems, ensure that the treatment variable is a factor in your data by transforming the column Treatment and Size via code similar to `data$Treatment <- factor(data$Treatment)`, otherwise the model will treat treatments 1 through 3 as integer values.

```{r}
# Transform data here
```

## Practice Problem 1

Create an unconditional (null) model with math score ("Y") as the outcome variable that uses "Animal" as the clustering variable. Use FIML as the estimator by inputting `REML = F` in the `lmer()` function. Then, compute the empty model's intraclass correlation with any method that we have used (`{performance}`, `VarCorr()`, or by hand). 

```{r}
# Create empty model and compute ICC
```

Considering the ICC, should we account for nested nature of the data? What proportion (or percent) of variance in the outcome is attributed to clustering?

Enter level 1 fixed effects for Treatment and Time. 

```{r}
# Create conditional model with new fixed effects
```

Why would treament and time be considered level 1 effects? What is "level 1" in this scenario?

Does the effect of treatment type on phlebitis (i.e., difference in ear temperatures) change based on time of measure? Insert a level 1 interaction fixed effect using either "*" or ":" syntax method from Practice4 to test this.

Plot the interaction via `{ggplot2}`. 

```{r}
# Interaction plot
```

Does effect of Treatment on Y depend on Time? Or, reversely, does effect of Time on Y depend on Treatment? 

Insert level 2 fixed effects "Age" and "Size". It was also hypothesized that the effect of time on differences in ear temperature depends on a dog's size. Insert a cross-level interaction to test this.

```{r}
# Model with Age and Size
```

What did the level 2 fixed effects tell us about a dog's age and size? 

Test whether model fit improved between the four models (unconditional, conditional with fixed effects, conditional with fixed effects and interaction, conditional model with level 1 and 2 fixed effects) via the `anova()` function.

---

## Practice Problem 2

```{r}
# Read in the reaction experiment data reactions.xlsx file with {readxl} package
```

The next faux dataset that we will use, reaction.xlsx, involves the use of a reaction time experiment conducted on 24 participants ("Subject"). 

Three pairs of the same images ("Cues") are randomized to appear on the screen that features a threat on either the right half of the computer screen or the left half of the copmuter screen. On the other half of the screen is a non-threatening image. Threatening images involved graphics of a masked robber pointing a gun at the participant, an attacking mountain lion, or a snake lunging toward the participant. Participants were given a three-second countdown in between each cue.

The same pairs of cues were randomly used twice -- once in color and once in black and white. Cue images that are in black and white were coded as Color = 0, whereas all images in full color were coded as Color = 1. 

Reaction time, in milliseconds, is measured by the time it takes for the participant to press down the "Q" (left-side) or "P" (right-side) keys to indicate which side of the screen the threat exists. 

For half of the sample, a distraction ("Disturb") occurred during the experiment that involved a randomly selected emoji that appears in the center of the screen when the images are revealed. 

Participants either correctly selected the threat (Correct = 1) or incorrectly assessed the threat (Correct = 0). 

Because this dataset involves a binary outcome (correct/incorrect), we must move into the **g**eneralized **l**inear **m**ixed **e**ffect **r**egression function, `glmer()`. This function comes from the same `{lme4}` package we have been using and also uses the exact same inputs as `lmer()` but allows for new families of distributions. For our case, the binomial distribution. See the code snippet below to see how new families of distributions are utilized:

```{r, eval = F}
glmer("typical lmer() formula", family = binomial, data = "data of interest")
```

Before building the generalized linear mixed effect model, transform the "Correct", "Disturb", "Cue", and "Color" variables from this dataset into factors instead of integer/numeric class variables. As done above, use syntax similar to `data$Correct <- factor(data$Correct)` to complete this.

```{r}
# Small variable transformation here
```

The research team wants to know if the ability to correctly identify the threatening image increases as number of images (Cues) are revealed. This is analagous to testing learning effects.

In addition, they would like to see if participants are better able to distinguish a threat when viewing colored images as opposed to black and white images. 

The last objective for the team is to investigate whether on-screen disturbances change the likelihood of selecting the threatening image. 

Using `glmer()`, create a mixed effect model, with grouping variable "Subject", that predicts whether or not the participant correctly identified the threatening image based on:

* If the image was in color
* Whether or not a disturbance occurred
* Whether the participant was better at identifying a threat after seeing the first first or second image (i.e., Cue)

```{r}
# First glmer() model for reactions data
```

To refer to effects in odds ratios (in place of log-odds), I would recommend using the `model_parameters()` function from the `parameters` package (which derives from the [easystats](https://github.com/easystats/easystats) collection) in place of exponentiating by hand or single function (e.g., exp(coefficient 1), exp(coefficient 2), etc.).  Use the below code to download this and run the proper function:

```{r, eval = F}
# Delete eval = F above
if(!require(parameters)){
    install.packages("parameters")
    library(parameters)
}

model_parameters("model name", exponentiate = T)
```

On average, do the odds of a participant selecting the threat significantly differ depending on whether the image is in color? If so, are participants more or less likely to detect a threat when the image is in color?

On average, do the odds of a participant selecting the threat significantly differ based on if there are on-screen disturbances? If so, are participants more or less likely to detect a threat when there are on-screen disturbances?

On average, do the odds of a participant selecting the threat significantly differ based on cue (number of images shown)? If so, are participants more or less likely to detect a threat during the second or third image?

Test whether the effect of an on-screen disturbance *moderates* the effect of colored images on correctly identifying the threatening image in a new `glmer()` object.

```{r}
# New glmer() model
```

Using the `lmer()` model, use the same set of explanatory variables used in the above model (including the interaction) to predict the amount of time it takes for the participant to press down a key (outcome variable = "Time"). 

```{r}
# New lmer() model
```

How do these explanatory variables explain or predict the length of time it takes for the participant to select the image on the screen? Were there differences in speed based on color of image, existing distractions, or number of cue?